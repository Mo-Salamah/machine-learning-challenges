{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# my model performance visualization file\n",
    "import performance_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "mnist = pd.read_csv('./data/mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = mnist.drop(columns='target')\n",
    "y = mnist['target']\n",
    "\n",
    "X_train = X.iloc[:60000, :]\n",
    "X_test = X.iloc[60000:, :]\n",
    "\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]\n",
    "\n",
    "# binary classification \n",
    "y_train_8 = y_train == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c1430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logestic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "cv = 5\n",
    "\n",
    "hyper_params = {'penalty':['none', 'l1', 'l2', 'elasticnet'],\n",
    "                # 'regularization':[],\n",
    "                'fit_intercept':[True, False],\n",
    "                'class_weight':['balanced', {0:1, 1:3}, {0:3, 1:1}], \n",
    "                # 'l1_ratio':[]\n",
    "                }\n",
    "\n",
    "rdm_grid_search_lr_clf = RandomizedSearchCV(lr_clf, hyper_params, cv=cv, return_train_score=True, )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2854282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search\n",
    "rdm_grid_search_lr_clf.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49670e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models failed to converge!\n",
    "# compare different models\n",
    "\n",
    "lr_clf_results = rdm_grid_search_lr_clf.cv_results_\n",
    "\n",
    "\n",
    "\n",
    "performances_lr_clf = zip(\n",
    "    lr_clf_results['params'], \n",
    "    lr_clf_results['mean_test_score'], \n",
    "    lr_clf_results['std_test_score'], \n",
    "    lr_clf_results['rank_test_score'])\n",
    "\n",
    "# print models performance scores\n",
    "for model_performance in performances_lr_clf:\n",
    "\n",
    "    params = model_performance[0]\n",
    "    mean_test_score = model_performance[1]\n",
    "    \n",
    "    print(mean_test_score, '   ', params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae22025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import clone\n",
    "\n",
    "copy the hyper parameters of the best estimator\n",
    "lr_clf_best = clone(rdm_grid_search_lr_clf.best_estimator_)\n",
    "\n",
    "# get the z_hat scores\n",
    "y_score_8_lr_clf = cross_val_predict(lr_clf_best, X_train, y_train_8, method=\"decision_function\")\n",
    "# now you can visualize performance using the other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91246c95",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualize the performance of the second best model\n",
    "# one with 'class_weight': {0: 1, 1: 3}\n",
    "y_score_8_lr_clf_whts = cross_val_predict(LogisticRegression(penalty= 'l2', fit_intercept= True, class_weight= {0: 1, 1: 3}),\n",
    "                                     X_train, y_train_8, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two models\n",
    "metrics_scores = performance_eval.performance_vs_thresholds(y_true=y_train_8, y_score=y_score_8_lr_clf)\n",
    "metrics_scores_whts = performance_eval.performance_vs_thresholds(y_true=y_train_8, y_score=y_score_8_lr_clf)\n",
    "\n",
    "\n",
    "performance_eval.plot_performance_curve(('fp/n', metrics_scores['fp/n']),\n",
    "                                        ('tp/p', metrics_scores['tp/p']), label='Logestic Regression')\n",
    "\n",
    "performance_eval.plot_performance_curve(('fp/n', metrics_scores['fp/n']),\n",
    "                                        ('tp/p', metrics_scores['tp/p']), label='Logestic Regression Weighted',\n",
    "                                        line_only=True)\n",
    "\n",
    "# ROC curves are exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d808fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# compare AUC scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score_lr_clf = roc_auc_score(y_train_8, y_score_8_lr_clf)\n",
    "auc_score_lr_clf_whts = roc_auc_score(y_train_8, y_score_8_lr_clf_whts)\n",
    "\n",
    "# auc scores for the second best model (according to grid search) were higher than best model!\n",
    "\n",
    "################################\n",
    "############ knn  ##############\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfc2ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "cv = 5\n",
    "\n",
    "params_knn = {\n",
    "    'n_neighbors':np.arange(1, 40, 2),\n",
    "}\n",
    "\n",
    "rdm_hyper_params_knn = {\n",
    "                'n_neighbors':[1, 5, 10], # would take way too long\n",
    "                'weights':['uniform', 'distance'],\n",
    "                # 'algorithm':['ball_tree', 'kd_tree' , 'brute', 'auto'],\n",
    "                'algorithm':['ball_tree', 'kd_tree', 'auto'],\n",
    "                \n",
    "                }\n",
    "\n",
    "rdm_grid_search_knn_clf = RandomizedSearchCV(knn_clf, rdm_hyper_params_knn, cv=cv, return_train_score=True, )\n",
    "grid_search_knn_clf = GridSearchCV(knn_clf, params_knn, cv=cv, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4076",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# run grid search\n",
    "rdm_grid_search_knn_clf.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f958d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grid_search_knn_clf.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualie performance for different k values\n",
    "\n",
    "scores_knn_clf = grid_search_knn_clf...\n",
    "x_ticks = params_knn.items()[0][1] # np.arange(1, 20, 2)\n",
    "plt.plot(x_ticks, scores_knn_clf, label='KNN Performance')\n",
    "plt.title('KNN Model Performance at Different K Values')\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel('Metric') # what is it?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models failed to converge!\n",
    "# compare different models\n",
    "\n",
    "rdm_knn_clf_results = rdm_grid_search_knn_clf.cv_results_\n",
    "\n",
    "\n",
    "\n",
    "performances_knn_clf = zip(\n",
    "    rdm_knn_clf_results['params'], \n",
    "    rdm_knn_clf_results['mean_test_score'], \n",
    "    rdm_knn_clf_results['std_test_score'], \n",
    "    rdm_knn_clf_results['rank_test_score'])\n",
    "\n",
    "# print models performance scores\n",
    "for model_performance in performances_knn_clf:\n",
    "\n",
    "    params = model_performance[0]\n",
    "    mean_test_score = model_performance[1]\n",
    "    \n",
    "    print(mean_test_score, '   ', params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7e80c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import clone\n",
    "\n",
    "copy the hyper parameters of the best estimator\n",
    "knn_clf_best = clone(rdm_grid_search_knn_clf.best_estimator_)\n",
    "\n",
    "# get the z_hat scores          # does knn algo have that?\n",
    "y_score_8_lr_clf = cross_val_predict(knn_clf_best, X_train, y_train_8, method=\"decision_function\")\n",
    "# now you can visualize performance using the other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec68c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualize the performance of the second best model\n",
    "# one with 'class_weight': {0: 1, 1: 3}\n",
    "y_score_8_lr_clf_whts = cross_val_predict(LogisticRegression(penalty= 'l2', fit_intercept= True, class_weight= {0: 1, 1: 3}),\n",
    "                                     X_train, y_train_8, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two models\n",
    "metrics_scores = performance_eval.performance_vs_thresholds(y_true=y_train_8, y_score=y_score_8_lr_clf)\n",
    "metrics_scores_whts = performance_eval.performance_vs_thresholds(y_true=y_train_8, y_score=y_score_8_lr_clf)\n",
    "\n",
    "\n",
    "performance_eval.plot_performance_curve(('fp/n', metrics_scores['fp/n']),\n",
    "                                        ('tp/p', metrics_scores['tp/p']), label='Logestic Regression')\n",
    "\n",
    "performance_eval.plot_performance_curve(('fp/n', metrics_scores['fp/n']),\n",
    "                                        ('tp/p', metrics_scores['tp/p']), label='Logestic Regression Weighted',\n",
    "                                        line_only=True)\n",
    "\n",
    "# ROC curves are exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare AUC scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score_lr_clf = roc_auc_score(y_train_8, y_score_8_lr_clf)\n",
    "auc_score_lr_clf_whts = roc_auc_score(y_train_8, y_score_8_lr_clf_whts)\n",
    "\n",
    "# auc scores for the second best model (according to grid search) were higher than best model!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae28d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualize ROC curve\n",
    "metrics_scores = performance_eval.performance_vs_thresholds(y_true=y_train_8, y_score=y_score_8_lr_clf)\n",
    "\n",
    "performance_eval.plot_performance_curve(('fp/n', metrics_scores['fp/n']),\n",
    "                                        ('tp/p', metrics_scores['tp/p']), label='Logestic Regression')\n",
    "\n",
    "# visualize other curves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3a195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
