{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57360325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# how to use\n",
    "# metrics_score = performance_vs_thresholds(...)\n",
    "# plot_performance_curve(metrics_score['fp/p'], metrics_score['tp/n'], label='Logestic Regression')\n",
    "# plot_performance_curve(..., line_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee077bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_NAMES = {\n",
    "    'tp/p': 'Recall',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f870fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# mnist = fetch_openml('mnist_784', version=1)\n",
    "# mnist.keys()\n",
    "# mnist = pd.read_csv('./data/mnist.csv')\n",
    "\n",
    "# # %%\n",
    "# # split to train and test sets\n",
    "\n",
    "# # X, y = mnist['data'], mnist['target']\n",
    "# X, y = mnist.drop(columns=['target']), mnist.iloc[:, -1]\n",
    "# X_train, X_test = X[:60000], X[60000:]\n",
    "# y_train, y_test = y[:60000].astype('int'), y[60000:].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(vector):\n",
    "    \"\"\"takes a 784 length vector and produces\n",
    "        a plot of the digit\n",
    "    Args:\n",
    "        vector (itterable): 784 length with floats from 0 to 255\n",
    "    \"\"\"\n",
    "    digit = np.array(vector).reshape((28, 28))\n",
    "    plt.imshow(digit, cmap='binary')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0db2ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualize one digit example\n",
    "# some_digit = X_train.iloc[0]\n",
    "# plot_digits(some_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify classification problem\n",
    "\n",
    "# two classes: true if 8, false otherwise\n",
    "# y_train_8 = (y_train == 8)\n",
    "# y_test_8 = (y_test == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classification model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "sgd_clf = SGDClassifier() # no parameters? what cost function is default?\n",
    "# sgd_clf.fit(X_train, y_train_8)\n",
    "# y_pred_8 = sgd_clf.predict(X_test)\n",
    "# cost = 'f1'\n",
    "# y_pred_8 = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand errors of the model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# conf_mtrx_8 = confusion_matrix(y_true=y_train_8, y_pred=y_pred_8)\n",
    "# targets_sums = np.sum(conf_mtrx_8, axis=1)\n",
    "# norm_conf_mtrx_8 = conf_mtrx_8 / targets_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d332ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4,4))\n",
    "# ConfusionMatrixDisplay.from_predictions(y_true=y_train_8, y_pred=y_pred_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculare different scoring metrics\n",
    "def assess_performance(y_true, y_pred, majority='negative'):\n",
    "    \"\"\"calculates the different classification \n",
    "    metrics for binary classification\n",
    "\n",
    "    Args:\n",
    "        y_true (itterable): array of {0, 1} \n",
    "        y_pred (itterable): array of predicted {0, 1}\n",
    "        majority (str)   : {'negative', 'positive'} designation of majority class\n",
    "    \"\"\"\n",
    "    conf_mtrx = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    sums = np.sum(conf_mtrx, axis=1)\n",
    "    majority_index = np.argmax(sums)\n",
    "    \n",
    "    if majority == 'positive':\n",
    "        positive_index = majority_index\n",
    "        if majority_index == 0:\n",
    "            negative_index = 1\n",
    "        else:\n",
    "            negative_index = 0 \n",
    "    else:\n",
    "        negative_index = majority_index\n",
    "        if majority_index == 0:\n",
    "            positive_index = 1\n",
    "        else:\n",
    "            positive_index = 0 \n",
    "        \n",
    "    # marginal sums\n",
    "    n_positive = np.sum(conf_mtrx[positive_index, :])\n",
    "    n_negative = np.sum(conf_mtrx[negative_index, :])\n",
    "    \n",
    "    n_pred_positive = np.sum(conf_mtrx[:, positive_index])\n",
    "    n_pred_negative = np.sum(conf_mtrx[:, negative_index])\n",
    "    \n",
    "    # positives\n",
    "    tp = conf_mtrx[positive_index, positive_index]\n",
    "    fn = conf_mtrx[positive_index, negative_index]\n",
    "    \n",
    "    # negatives\n",
    "    tn = conf_mtrx[negative_index, negative_index]\n",
    "    fp = conf_mtrx[negative_index, positive_index]\n",
    "        \n",
    "    metrics = {}\n",
    "    \n",
    "    # recall\n",
    "    metrics['tp/p'] = tp / n_positive\n",
    "    # 1 - recall\n",
    "    metrics['fn/p'] = fn / n_positive\n",
    "    \n",
    "    # 1 - percision\n",
    "    metrics['fp/g'] = fp / n_pred_positive\n",
    "    # percision\n",
    "    metrics['tp/g'] = tp / n_pred_positive\n",
    "    \n",
    "    # specificity\n",
    "    metrics['tn/n'] = tn / n_negative\n",
    "    # 1 - specificity\n",
    "    metrics['fp/n'] = fp / n_negative\n",
    "    \n",
    "    # !!\n",
    "    metrics['tn/b'] = tn / n_pred_negative\n",
    "    metrics['fn/b'] = fn / n_pred_negative\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0d7fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# metrics_sgd_8 = assess_performance(y_train_8, y_pred_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5f1db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# calculate error metrics v.s. different threshholds \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# y_score_8 = cross_val_predict(sgd_clf, X_train, y_train_8, \n",
    "#                               cv=3, method=\"decision_function\")\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_true=y_train_8, y_score=y_score_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da736889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def performance_vs_thresholds(y_true,\n",
    "                              y_score, \n",
    "                              metric1='tp/p', metric2='fp/n'):\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    \n",
    "    # {'tp/p':[0.93, 0. 84,...]}\n",
    "    metrics_scores = {'thresholds':thresholds, metric1:[], metric2:[]}\n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        # positive class if y_score[i] > threshold, for each element in y\n",
    "        y_pred = y_score > threshold\n",
    "        \n",
    "        # performance metrics at threshold\n",
    "        metrics = assess_performance(y_true, y_pred)\n",
    "        per1, per2 = metrics[metric1], metrics[metric2]\n",
    "        \n",
    "        metrics_scores[metric1].append(per1)\n",
    "        metrics_scores[metric2].append(per2)\n",
    "        \n",
    "    return metrics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1d42d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualize error metrics and threshhold\n",
    "def plot_performance_curve(metric1, metric2, label=None, line_only=False,\n",
    "                           use_conventional_names=False):\n",
    "    \"\"\"generates a plot similar to the ROC curve based\n",
    "    on model's performance at different thresholds\n",
    "\n",
    "    Args:\n",
    "        metric1 (tuple): 'metric_name', [...metric scores...]\n",
    "        metric2 (tuple): _description_\n",
    "        label (str)    : e.g., 'Random Forest'\n",
    "    \"\"\"\n",
    "    \n",
    "    metric1_name, metric1_score = metric1\n",
    "    metric2_name, metric2_score = metric2\n",
    "    \n",
    "    # e.g., from 'tp/p' to 'Recall'\n",
    "    if use_conventional_names:\n",
    "        metric1_name = METRICS_NAMES[metric1_name]\n",
    "        metric2_name = METRICS_NAMES[metric2_name]\n",
    "    \n",
    "    # set color and linestyle randomly to avoid using the same \n",
    "    # color and style repeatedly\n",
    "    # linestyles = ['-', '--', '-.', ':']\n",
    "    linestyles = ['-']\n",
    "    \n",
    "    color = sb.palettes.color_palette()[np.random.randint(0, 10)]\n",
    "    linestyle = linestyles[np.random.randint(0, len(linestyles))]\n",
    "    \n",
    "    \n",
    "    plt.plot(metric1_score, metric2_score, label=label,\n",
    "             color=color, linestyle=linestyle)\n",
    "    \n",
    "    if not line_only:\n",
    "        plt.xlabel(metric1_name)\n",
    "        plt.ylabel(metric2_name)\n",
    "        plt.title(f\"{metric1_name} v.s. {metric2_name}\")\n",
    "        plt.grid()\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0cd4e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
